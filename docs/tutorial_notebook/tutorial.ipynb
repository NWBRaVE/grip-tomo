{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Notebook\n",
    "This is a tutorial notebook that goes through the GRIP-Tomo 2.0 pipeline for converting the MRC format cryo-ET sub-tomograms into clusters, graphs\n",
    "and graph features, then using the graph features to perform machine learning classifications. \n",
    "\n",
    "The MRC -> graph -> features results in the manuscript were done by a using hardware-specific parsl parallel computing on NERSC perlmutter platform. \n",
    "The pipeline shown in this tutorial notebook uses a serial example to show how the pipeline works (without parsl or parallel computing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRC -> graph -> features\n",
    "For a single example MRC and feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules imported\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import hdbscan\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# import grip-tomo modules\n",
    "import griptomo.core.pdb2graph as p2g\n",
    "import griptomo.core.density2graph as d2g\n",
    "import griptomo.core.graph2class as g2c\n",
    "\n",
    "# plot settings\n",
    "plt.rc('axes', axisbelow=True)\n",
    "px = 1/72\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('modules imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MRC format cryo-ET sub-tomogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the mrc file with the package mrcfile\n",
    "import mrcfile\n",
    "mrc = mrcfile.mmap('108-382-131.mrc', mode='r') # apoferritin example mrc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the mrc data (no header, etc.) into a new object\n",
    "mrc_sliced = mrc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219, 219, 219)\n"
     ]
    }
   ],
   "source": [
    "# check dimensions of the loaded data\n",
    "print (mrc_sliced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing before constructing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization/standarization of densities\n",
    "normalized_densities = d2g.normalize_mrc_data(mrc_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and apply a threshold (which is ratio in this notebook, 0.9 means only top 10% densities are preserved) to filter out low density signals\n",
    "threshold_ratio = 0.9\n",
    "threshold = d2g.identify_threshold_ratio(normalized_densities, threshold_ratio)\n",
    "point_cloud = d2g.generate_point_cloud_from_mrc_data(normalized_densities, threshold) \n",
    "density_map = normalized_densities[point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2]] + 1\n",
    "density_map = density_map / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precoarsening the densities\n",
    "precoarsened_point_cloud, precoarsened_density = d2g.voxel_coarsening(3, point_cloud, np.zeros(point_cloud.shape[0], dtype=np.int64), density_map, averaged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN clustering\n",
    "model_clusters = hdbscan.HDBSCAN(min_cluster_size=512, min_samples=512, allow_single_cluster=False, core_dist_n_jobs=-1, approx_min_span_tree=False, cluster_selection_epsilon=0, cluster_selection_method='eom').fit(precoarsened_point_cloud)\n",
    "labels = model_clusters.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct and save graph at cutoff distance 5 angstrom as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved with 20071 nodes and 124432 edges.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "# Use non-noise points only\n",
    "coords = precoarsened_point_cloud\n",
    "labels = model_clusters.labels_\n",
    "\n",
    "# Filter to valid (non-noise) points\n",
    "valid_idx = np.where(labels >= 0)[0]\n",
    "valid_coords = coords[valid_idx]\n",
    "valid_labels = labels[valid_idx]\n",
    "\n",
    "# KDTree for valid points\n",
    "cutoff_distance = 5\n",
    "tree = KDTree(valid_coords)\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add valid nodes with x, y, z, cluster attributes\n",
    "for i_local, i_global in enumerate(valid_idx):\n",
    "    x, y, z = coords[i_global]\n",
    "    G.add_node(i_global, x=float(x), y=float(y), z=float(z), cluster=int(labels[i_global]))\n",
    "\n",
    "# Add edges among valid nodes based on cutoff distance\n",
    "for i_local, i_global in enumerate(valid_idx):\n",
    "    neighbors = tree.query_ball_point(coords[i_global], r=cutoff_distance)\n",
    "    for j_local in neighbors:\n",
    "        if i_local < j_local:\n",
    "            j_global = valid_idx[j_local]\n",
    "            dist = np.linalg.norm(coords[i_global] - coords[j_global])\n",
    "            G.add_edge(i_global, j_global, weight=dist)\n",
    "\n",
    "# Save the graph\n",
    "nx.write_gexf(G, \"./graph_dcut=5.gexf\")\n",
    "print(f\"Graph saved with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate features of the graph and print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n nodes': 14106.0, 'n edges': 91492.0, 'density': 0.0009196787396866979, 'diameter': 97, 'avg clustering': 0.4992673822878274, 'max closeness centrality': 0.047121433320081646, 'max eigenvector centrality': 0.1308360599223414, 'degree assortativity': 0.741563316369559, 'max clique number': 9, 'n communities': 15, 'avg path length': 29.924585700016884, 'max betweenness centrality': 0.17719938220658}\n"
     ]
    }
   ],
   "source": [
    "# This make take a few minutes depending on the graph size (more time when graph is large)\n",
    "G_feat = g2c.igraph_calc_graph_features(G)\n",
    "print (G_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section a pipeline of machine learning classification is shown using the data obtained from NERSC Perlmutter. The training and testing sets are the combination of graph features of MRC samples in various cutoff distances, i.e. hundreds of feature vectors calculated in the pipeline above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics       import (\n",
    "    accuracy_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings of plots and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the font as Arial for all figures\n",
    "import matplotlib as mpl\n",
    "\n",
    "#set up the font type as arial\n",
    "font = {'family' : 'Arial',\n",
    "#         'weight' : 'light'}\n",
    "#         'weight' : 'light',\n",
    "        'size'   : 20}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "dpi        = 1000\n",
    "top_n      = 10\n",
    "\n",
    "# Load training and testing data\n",
    "train_path = './combined_features_mock_training_cleaned_concatenated.csv'\n",
    "test_path  = './combined_features_experimental_testing_cleaned_concatenated.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge apoferritin and beta-gal classes into a single 'proteins' class, skip if not binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two proteins classes → single “proteins”\n",
    "mapping = {\n",
    "    'horse-l-apoferritin': 'proteins',\n",
    "    'betagal':            'proteins',\n",
    "}\n",
    "train_df = train_df[train_df['category'] != 'aldolase']\n",
    "train_df['category'] = train_df['category'].replace(mapping)\n",
    "test_df ['category'] = test_df ['category'].replace(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out features & labels\n",
    "features = [c for c in train_df.columns if c not in ('file_name','category')]\n",
    "X_train, y_train = train_df[features], train_df['category']\n",
    "X_test,  y_test  = test_df [features], test_df ['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of classification performance and make plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest ===\n",
      "Accuracy:    0.8103\n",
      "Weighted F1: 0.7900\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       noise       0.90      0.46      0.61        56\n",
      "    proteins       0.79      0.97      0.87       118\n",
      "\n",
      "    accuracy                           0.81       174\n",
      "   macro avg       0.84      0.72      0.74       174\n",
      "weighted avg       0.83      0.81      0.79       174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics including accuracy, F1-score, confusion matrix and feature importance, etc\n",
    "\n",
    "# accuracy\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "# weighted F1-score\n",
    "f1_rf  = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "# Print performance\n",
    "print(\"=== Random Forest ===\")\n",
    "print(f\"Accuracy:    {acc_rf:.4f}\")\n",
    "print(f\"Weighted F1: {f1_rf:.4f}\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf, labels=rf.classes_)\n",
    "classes = rf.classes_\n",
    "n = len(classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5), dpi=dpi)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        face = 'black' if i==j else 'white'\n",
    "        txtc = 'white' if i==j else 'black'\n",
    "        rect = plt.Rectangle((j, i), 1, 1, facecolor=face, edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(j+0.5, i+0.5, cm[i,j],\n",
    "                ha='center', va='center', color=txtc, fontsize=16)\n",
    "# ticks & labels\n",
    "ax.set_xticks(np.arange(n)+0.5)\n",
    "ax.set_yticks(np.arange(n)+0.5)\n",
    "ax.set_xticklabels(classes, fontsize=20)\n",
    "ax.set_yticklabels(classes, fontsize=20)\n",
    "ax.set_xlim(0, n)\n",
    "ax.set_ylim(n, 0)\n",
    "ax.set_xlabel('Predicted', fontsize=20)\n",
    "ax.set_ylabel('Actual', fontsize=20)\n",
    "ax.set_title('RF Confusion Matrix', fontsize=18, pad=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./rf_confusion_matrix.png', dpi=dpi)\n",
    "plt.close()\n",
    "\n",
    "# Plot feature importance\n",
    "importances = rf.feature_importances_\n",
    "idxs = np.argsort(importances)[::-1][:top_n]\n",
    "\n",
    "def prettify_feature_name(name):\n",
    "    match = re.search(r'(.*)_cutoff_(\\d+)', name)\n",
    "    if match:\n",
    "        base, dcut = match.groups()\n",
    "        return fr\"{base} ($d_{{\\mathrm{{cut}}}}^{{\\prime}}={dcut}$)\"\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "pretty_names = [prettify_feature_name(features[i]) for i in idxs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4), dpi=dpi)\n",
    "ax.barh(range(top_n)[::-1], importances[idxs][::-1], align='center')\n",
    "ax.set_yticks(range(top_n)[::-1])\n",
    "ax.set_yticklabels(pretty_names[::-1], fontsize=14)\n",
    "ax.set_xlabel('Importance', fontsize=16)\n",
    "ax.set_title('RF Feature Importances (Top 10)', fontsize=18, pad=12)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./rf_feature_importance.png', dpi=dpi, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
